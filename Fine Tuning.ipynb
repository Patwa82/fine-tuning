{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe25067",
   "metadata": {},
   "source": [
    "# install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c9d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b96046",
   "metadata": {},
   "source": [
    "# import all required libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb4823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import(AutoModelForCausalLM,\n",
    "                        AutoTokenizer,\n",
    "                        BitsAndBytesConfig,\n",
    "                        HfArgumentParser,\n",
    "                        TrainingArguments,\n",
    "                        pipeline,\n",
    "                        logging,\n",
    "                )\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca7287",
   "metadata": {},
   "source": [
    "# in LLma2, following promt template is used for chat model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0395286f",
   "metadata": {},
   "source": [
    "System prompt(optional) to guide the model,\n",
    "user promt(required),\n",
    "answer the model(required)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4529edd",
   "metadata": {},
   "source": [
    "<s> [INST] <<SYS>>\n",
    "System prompt\n",
    "<</SYS>>\n",
    "user promt[/INST] Model answer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9309da17",
   "metadata": {},
   "source": [
    "reformat our instruction datasets to follow Llama2 template\n",
    "Original: https://huggingface.co/datasets/timdettmers/openassistant-guanaco\n",
    "Reformat: https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k/tree/main\n",
    "complete reformate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dad8f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model that you want to train from the hugging face hub\n",
    "model_name=\"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# instruct dataset to use \n",
    "dataset_name=\"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# fined tuned model name\n",
    "new_model=\"Llama-2-7b-chat-finetune\"\n",
    "\n",
    "# QloRA Parameters\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r=64\n",
    "\n",
    "# Alpha parameter for LoRA scaling \n",
    "lora_alpha=16\n",
    "\n",
    "# Dropout probability for LoRA layer\n",
    "\n",
    "lora_dropout=0.1\n",
    "\n",
    "# bitsandbytes parameters \n",
    "# activate 4-bit precision base model loading\n",
    "\n",
    "use_4bit=True\n",
    "\n",
    "# compute dtype for 4-bit models\n",
    "bnb_4bit_compute_dtype=\"float16\"\n",
    "\n",
    "# quantization type (fp4 0r nf4)\n",
    "bnb_4bit_quant_type=\"nf4\"\n",
    "\n",
    "# activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant=False\n",
    "\n",
    "\n",
    "## training arguments parameters\n",
    "\n",
    "# output directory where the model predictions and checkpoints will be stored\n",
    "output_dir=\"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs=1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True  with an A100)\n",
    "fp16=False\n",
    "bf16=False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size=4\n",
    "\n",
    "# batch size per GPU for evaluation\n",
    "per_device_eval_batch_size=4\n",
    "\n",
    "# Number of update steps to accumate the gradient for \n",
    "gradient_accumation_steps=1\n",
    "\n",
    "#Enable gradient checkpointing\n",
    "gradient_checkpointing=True\n",
    "\n",
    "# Maximum gradient normal (gradient Clipping)\n",
    "max_grad_norm=0.3\n",
    "\n",
    "# Initial Learning rate (AdamW Optimizer)\n",
    "learning_rate=2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay=0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim=\"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type=\"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_step=-1\n",
    "\n",
    "# Ratio of steps for linear warmup (from 0 to learning rate)\n",
    "warmup_ratio=0.03\n",
    "\n",
    "# Group sequences into batches with same lenght\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length=True\n",
    "\n",
    "# save checkpoints every X updates steps \n",
    "save_steps=0\n",
    "\n",
    "# Log every X updates steps \n",
    "logging_steps=25\n",
    "\n",
    "# SFT parameters \n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length=None\n",
    "\n",
    "# pack multiple short examples in the same inputs sequence to increase efficiency \n",
    "packing=False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map={\"\":0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb02d38",
   "metadata": {},
   "source": [
    "# Load Everything and start the fine_tuning process"
   ]
  },
  {
   "cell_type": "raw",
   "id": "197b3234",
   "metadata": {},
   "source": [
    "1. we want to load dataset we defined . Here our datasets is already preprocessed but usually this is where you would \n",
    "reformat the prompt, filter out bad text,combine multiple datasets, etc.\n",
    "2. Then, we are configuring bitsandBytes for 4-bit quantization.\n",
    "3. Next, we are loading the llama 2 model in 4-bit precision on GPU with the corresponding tokenization\n",
    "4. Finally, we are loading configuration for QLoRA, regular training parameters ans passing everything to the SFTTrainer the train can finally start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02e34a0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/Asus/.cache/huggingface/datasets/mlabonne___parquet/mlabonne--guanaco-llama2-1k-f1f1134768f90029/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Check GPU compatibility with bfloats16\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_dtype\u001b[38;5;241m==\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;129;01mand\u001b[39;00m use_4bit:\n\u001b[1;32m---> 15\u001b[0m     major,_\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_capability()\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m major\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m:\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\__init__.py:439\u001b[0m, in \u001b[0;36mget_device_capability\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_capability\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    427\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m        tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m     prop \u001b[38;5;241m=\u001b[39m get_device_properties(device)\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\__init__.py:453\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \n\u001b[0;32m    446\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 453\u001b[0m     _lazy_init()  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    454\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\cuda\\__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    301\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 302\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_init()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[0;32m    306\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# Load Datasets (you can process it here)\n",
    "dataset=load_dataset(dataset_name,split=\"train\")\n",
    "\n",
    "# Load Tokenizer and model with QLoRA configuration \n",
    "compute_dtype=getattr(torch,bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "     load_in_4bit=use_4bit,\n",
    "     bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "     bnb_4bit_compute_dtype=compute_dtype,\n",
    "     bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "# Check GPU compatibility with bfloats16\n",
    "if compute_dtype==torch.float16 and use_4bit:\n",
    "    major,_=torch.cuda.get_device_capability()\n",
    "    if major>=8:\n",
    "        print(\"=\"*80)\n",
    "        print(\"Your GPU Support bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "# Load base model\n",
    "model=AutoModelForCasualLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache=False\n",
    "model.config.pretraining_tp=1\n",
    "\n",
    "## Load LLaMa tokenizer\n",
    "tokenizer=AutoTokenizer.from_pretarined(model_name,trust_remote_code=True)\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.padding_side=\"right\" # Fix weired overflow issue with fp16 training\n",
    "\n",
    "# Load loRA configuration\n",
    "peft_config=LoraConfig(\n",
    "     lora_alpha=lora_alpha,\n",
    "     loar_dropout=lora_dropout,\n",
    "     r=lora_r,\n",
    "     bias=\"none\",\n",
    "     task_type=\"CASUAL_LM\"\n",
    ")\n",
    "\n",
    "#set training parameters\n",
    "training_arguments=Training_Arguments(\n",
    "      output_dir=output_dir,\n",
    "      num_train_epochs=num_train_epochs,\n",
    "      per_device_train_batch_size=per_device_train_batch_size,\n",
    "      gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "      optim=optim,\n",
    "      save_steps=save_steps,\n",
    "      logging_steps=logging_steps,\n",
    "      learning_rate=learning_rate,\n",
    "      weight_decay=weight_decay,\n",
    "      fp16=fp16,\n",
    "      bf16=bf16,\n",
    "      max_grad_norm=max_grad_norm,\n",
    "      max_steps=max_steps,\n",
    "      warmup_ratio=warmup_ratio,\n",
    "      group_by_length=group_by_length,\n",
    "      lr_scheduler_type=lr_scheduler_type,\n",
    "      report_to=\"tensorboard\"\n",
    ")\n",
    "# set supervised fine tuning parameters\n",
    "trainer=SFTTrainer(\n",
    "       model=model,\n",
    "       train_dataset=dataset,\n",
    "       peft_config=peft_config,\n",
    "       dataset_text_field=\"text\",\n",
    "       max_seq_length=max_seq_length,\n",
    "       tokenizer=tokenizer,\n",
    "       args=training_arguments,\n",
    "       packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
